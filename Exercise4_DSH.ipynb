{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807542a5-6829-4c85-80e6-94c8ff5bcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Machine learning classifiers examples in healthcare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf9ef5c3-3ca9-4b92-a136-4081fc63060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Exploratory data analysis\n",
    "\n",
    "# 1.1 Reading dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "colnames = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\n",
    "df = pd.read_csv('diabetes.csv', skiprows = 1, header = None, names = colnames)\n",
    "#Dataset contains different measures and based on them, patient suffers from diabetes or not.\n",
    "#With seaborn.boxplot we can plot different variables and see how the variable is distributed.\n",
    "#Interpreting plots we can predict whether a patient has a diabetes or not.\n",
    "#Plotting many variables same time interpreting becomes easier, since we can study how the\n",
    "#variables are connected to each other. For instance, if patient has a high blood pressure, has a high BMI-index\n",
    "#and glucose levels are high, we can estimate that he/she might have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fdea3896-c2ad-43b5-b470-b12f7999ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients that do not suffer from diabetes: 500\n",
      "Number of patients that suffer from diabetes: 268\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Target feature's value distribution\n",
    "df_outcome = df['Outcome']\n",
    "outcome_val = df_outcome.value_counts()\n",
    "print('Number of patients that do not suffer from diabetes:', outcome_val[0])\n",
    "print('Number of patients that suffer from diabetes:', outcome_val[1])\n",
    "#Target feature ('Outcome': Patient suffers from diabetes (1) or not (0)). \n",
    "# From the 768 instances, 268 has a diabetes and 500 don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5193cc0c-ab90-46cd-97ee-9d110b723ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.3 Revealing outliers\n",
    "#df_pregnancies = sns.boxplot(x = df['Pregnancies'])\n",
    "#df_glucose = sns.boxplot(x = df['Glucose'])\n",
    "#df_bp = sns.boxplot(x = df['BloodPressure'])\n",
    "#df_st = sns.boxplot(x = df['SkinThickness'])\n",
    "#df_insulin = sns.boxplot(x = df['Insulin'])\n",
    "df_bmi = sns.boxplot(x = df['BMI'])\n",
    "#df_dpf = sns.boxplot(x = df['DiabetesPedigreeFunction'])\n",
    "#df_age = sns.boxplot(x = df['Age'])\n",
    "\n",
    "#With seaborn.boxplot we can reveal variable outliers. Plot shows distribution of the variable and\n",
    "#so we can detect deviations. For example BMI boxplot below shows the distribution. We can see\n",
    "#that some patients have very low BMI (under 20) and some have very high BMI (over 45).\n",
    "#those tailvalues represent the outliers of the BMI variable. Other variables/features have\n",
    "#also tailvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ca557-b8c1-4fd0-984a-3ca34980178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Feature scaling\n",
    "#We could use rescaling, in which new value is calculated via minimum and maximum of the feature \n",
    "#dataset. In addition, z-score calculations would help to scale features to 0-1 scale.\n",
    "#Logistic regression would be a good method for rescaling also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e16814df-6746-4ce6-b6c3-2b44c0a29dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report has been saved to extensive_EDA.html!\n"
     ]
    }
   ],
   "source": [
    "# 2. Extensive EDA\n",
    "from dataprep.eda import create_report\n",
    "report = create_report(df)\n",
    "report.save('extensive_EDA')\n",
    "# 2.1\n",
    "#We can see from the generated extensive_EDA.html file that the report gives additional information\n",
    "#about every variable and shows how much memory it takes. Moreover, report gives statistics about \n",
    "#dataset and insights. Interesting is that the report is pretty comprehensive and detailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a71d39da-2d9f-4de6-be35-6358be8f949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8)\n",
      "(231, 8)\n",
      "(537,)\n",
      "(231,)\n",
      "0    150\n",
      "1     81\n",
      "Name: Outcome, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. Building the classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 3.1 Splitting the dataset into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.3, train_size = 0.7, random_state = 42, stratify = df['Outcome'])\n",
    "X_train = train.drop('Outcome',axis=1)\n",
    "y_train = train['Outcome'].copy()\n",
    "X_test = test.drop('Outcome',axis=1)\n",
    "y_test = test['Outcome'].copy()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# 3.2 Stratification\n",
    "#In data-analysis stratification means that each set is represented equally.\n",
    "print(pd.value_counts(y_test)) #For instance, the y_test dataset contains roughly\n",
    "#equal amount of zeros and ones (150 and 81). It is important that datasets are\n",
    "#comparable and fair with equal amounts of data.\n",
    "\n",
    "# 3.3 Preprocessing X_train dataset\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train, y_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "#StandardScaler is good option to scale features to the same range. Another option \n",
    "#in scikit-learn function would be PCA or normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "44210b59-a136-4b58-a8a6-b423b6a3f67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurary of predictive model (Random Forest): 0.7489177489177489\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Fitting pipeline with training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "rf_clf = RandomForestClassifier(max_depth=5, n_estimators=100, min_samples_split=8,\n",
    "min_samples_leaf = 3, max_features= 'auto', bootstrap= True, random_state=42)\n",
    "std_scaler = StandardScaler()\n",
    "clf_pipe = Pipeline([('scaler', std_scaler), ('clf', rf_clf)])\n",
    "clf_pipe.fit(X_train, y_train)\n",
    "print('Accurary of predictive model (Random Forest):',clf_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a2d2d5e7-22db-4dcb-ac6b-ac3af085284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model score: 0.7445887445887446\n",
      "Support vector machine model score: 0.7359307359307359\n",
      "Decision tree model score: 0.7619047619047619\n",
      "Adaptive boosting model score: 0.7402597402597403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksisormunen/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time params  \\\n",
      "0       0.081384       0.01026         0.008941        0.000802     {}   \n",
      "\n",
      "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0           0.722222           0.787037           0.785047           0.728972   \n",
      "\n",
      "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.728972          0.75045        0.029172                1  \n"
     ]
    }
   ],
   "source": [
    "# 4. Comparing several classifiers\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression(random_state = 42)\n",
    "lr_clf_pipe = Pipeline([('scaler', std_scaler), ('clf', lr_clf)])\n",
    "lr_clf_pipe.fit(X_train, y_train)\n",
    "print('Logistic regression model score:',lr_clf_pipe.score(X_test,y_test))\n",
    "\n",
    "#Support vector machine (Linear Kernel) \n",
    "from sklearn.svm import LinearSVC\n",
    "svc_clf = LinearSVC(random_state = 42)\n",
    "svc_clf_pipe = Pipeline([('scaler', std_scaler), ('clf', svc_clf)])\n",
    "svc_clf_pipe.fit(X_train, y_train)\n",
    "print('Support vector machine model score:',svc_clf_pipe.score(X_test,y_test))\n",
    "\n",
    "#Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc_clf = DecisionTreeClassifier(random_state = 42)\n",
    "dc_clf_pipe = Pipeline([('scaler', std_scaler), ('clf', dc_clf)])\n",
    "dc_clf_pipe.fit(X_train, y_train)\n",
    "print('Decision tree model score:',dc_clf_pipe.score(X_test,y_test))\n",
    "\n",
    "#Adaptive boosting\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab_clf = AdaBoostClassifier(random_state = 42)\n",
    "ab_clf_pipe = Pipeline([('scaler', std_scaler), ('clf', ab_clf)])\n",
    "ab_clf_pipe.fit(X_train, y_train)\n",
    "print('Adaptive boosting model score:',ab_clf_pipe.score(X_test,y_test))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {}\n",
    "grid = GridSearchCV(estimator = ab_clf, param_grid = parameters, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "df_results = pd.DataFrame(grid.cv_results_)\n",
    "print(df_results)\n",
    "# 4.1\n",
    "#We can see that decision tree model score is the most accurate model.\n",
    "#Logistic regression is close to random forest model accurary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "98adc501-d1dd-40dd-bbaf-f2bf8f9281d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time params  \\\n",
      "0       0.081384       0.01026         0.008941        0.000802     {}   \n",
      "\n",
      "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0           0.722222           0.787037           0.785047           0.728972   \n",
      "\n",
      "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.728972          0.75045        0.029172                1  \n"
     ]
    }
   ],
   "source": [
    "# 5. Bonus\n",
    "from openpyxl import Workbook\n",
    "df_results_new_df = pd.DataFrame(df_results)\n",
    "print(df_results_new_df)\n",
    "df_results_new_df.to_excel(r'df_results_bonus.xlsx', index=False, engine = 'openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645d7e9-3b35-48ca-ad12-a1576fc5ac58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
